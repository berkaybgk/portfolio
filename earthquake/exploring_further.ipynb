{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T14:35:22.878578Z",
     "start_time": "2025-01-24T14:35:22.873722Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:32.860712Z",
     "start_time": "2025-01-24T16:53:32.851127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarthquakeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(EarthquakeLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Define output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        # mask shape: (batch_size, seq_length)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Get sequence lengths from mask\n",
    "        lengths = mask.sum(dim=1).cpu()\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_out, (hidden, _) = self.lstm(packed_x, (h0, c0))\n",
    "        \n",
    "        # Unpack sequence\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # Get the last valid output for each sequence using the mask\n",
    "        last_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            last_idx = lengths[i] - 1\n",
    "            last_outputs.append(out[i, last_idx])\n",
    "            \n",
    "        # Stack the last outputs\n",
    "        last_outputs = torch.stack(last_outputs)\n",
    "        \n",
    "        # Apply dropout\n",
    "        last_outputs = self.dropout(last_outputs)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.fc(last_outputs)\n",
    "        \n",
    "        return output\n"
   ],
   "id": "1445725921814dc3",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:33.545675Z",
     "start_time": "2025-01-24T16:53:33.538168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess(csv_path):\n",
    "    \"\"\"\n",
    "    Load the earthquake dataset and preprocess it.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the earthquake dataset CSV file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed earthquake dataset\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Drop the imbalanced magnitudes\n",
    "    df.drop(df[df[\"mag\"] < 4.0].index, inplace=True)\n",
    "\n",
    "    # Sort the dfFrame by time\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"].str[:10])\n",
    "\n",
    "    # Extract date features\n",
    "    df['year'] = df['time'].dt.year\n",
    "    df['month'] = df['time'].dt.month\n",
    "    df['day'] = df['time'].dt.day\n",
    "\n",
    "    # Drop columns with too many missing values\n",
    "    df.drop(columns=[\"dmin\", \"horizontalError\", \"depthError\", \"magError\"], inplace=True)\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in [\"nst\", \"gap\", \"magNst\", \"rms\"]:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "\n",
    "    # We can drop the id, updated, type, place, magNst, status, net, locationSource,\n",
    "    # and magSource columns as they are not informative\n",
    "    df.drop(columns=[\"id\", \"updated\", \"type\", \"place\", \"magNst\",\n",
    "                       \"status\", \"locationSource\", \"magSource\", \"net\",\n",
    "                     \"nst\", \"gap\", \"rms\", \"magType\" ], inplace=True)\n",
    "\n",
    "    # Create a target variable\n",
    "    df['target'] = 0  # Initialize target column\n",
    "\n",
    "    # Create the target variable\n",
    "    for i in range(len(df) - 1):\n",
    "        current_time = df.loc[i, 'time']\n",
    "        # Filter events within the next two weeks and within 0.5 degrees proximity\n",
    "        mask = (df['time'] > current_time) & (df['time'] <= current_time + pd.Timedelta(weeks=2)) & (\n",
    "                    np.abs(df['latitude'] - df.loc[i, 'latitude']) <= 0.5) & (\n",
    "                           np.abs(df['longitude'] - df.loc[i, 'longitude']) <= 0.5)\n",
    "\n",
    "        # If there is at least one significant earthquake, set the target to 1\n",
    "        if df[mask]['mag'].max() >= 5.0:\n",
    "            df.loc[i, 'target'] = 1\n",
    "\n",
    "    return df\n"
   ],
   "id": "c6a764ea4c81b6b",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:34.245702Z",
     "start_time": "2025-01-24T16:53:34.239511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_recent_earthquakes(data, mag_threshold = 4.5, proximity_threshold = 0.8, time_threshold = 2):\n",
    "    \"\"\"\n",
    "    Find recent earthquakes within 0.8 degrees latitude/longitude and 2 weeks of the current earthquake.\n",
    "    Add a new column `close_event` to the DataFrame with the total number of recent earthquakes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Full earthquake dataset\n",
    "    mag_threshold : float\n",
    "        Minimum magnitude of the earthquake to be considered\n",
    "    proximity_threshold : float\n",
    "        Maximum distance in degrees latitude/longitude to be considered\n",
    "    time_threshold : int\n",
    "        Maximum number of weeks to be considered\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Earthquake dataset with a new column `close_event` indicating if there was a recent earthquake nearby\n",
    "    \"\"\"\n",
    "    time_threshold = pd.Timedelta(weeks=time_threshold)\n",
    "\n",
    "    # Ensure the DataFrame is sorted by time for efficiency\n",
    "    data = data.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    # Initialize a new column\n",
    "    data[\"close_event\"] = 0\n",
    "\n",
    "    # Iterate through each row\n",
    "    for i, row in data.iterrows():\n",
    "        # Define current earthquake properties\n",
    "        current_time = row[\"time\"]\n",
    "        current_lat = row[\"latitude\"]\n",
    "        current_lon = row[\"longitude\"]\n",
    "\n",
    "        # Find earthquakes that meet the criteria\n",
    "        recent_quakes = data[\n",
    "            (data[\"time\"] < current_time) &\n",
    "            (data[\"time\"] >= current_time - time_threshold) &\n",
    "            (np.abs(data[\"latitude\"] - current_lat) <= proximity_threshold) &\n",
    "            (np.abs(data[\"longitude\"] - current_lon) <= proximity_threshold) &\n",
    "            (data[\"mag\"] > mag_threshold)\n",
    "            ]\n",
    "\n",
    "        # Update the new column\n",
    "        data.loc[i, \"close_event\"] = recent_quakes.shape[0]\n",
    "\n",
    "    return data"
   ],
   "id": "d1a9b6575effb8e0",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:36.365639Z",
     "start_time": "2025-01-24T16:53:34.971928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = load_and_preprocess(\"resources/90_25_turkey.csv\")\n",
    "\n",
    "print(df.describe())"
   ],
   "id": "6d15a2f1c9a61498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                time     latitude    longitude        depth  \\\n",
      "count                           4128  4128.000000  4128.000000  4128.000000   \n",
      "mean   2010-10-01 20:57:12.558139392    38.171928    33.770976    19.450909   \n",
      "min              1990-01-02 00:00:00    35.647300    25.510900     0.000000   \n",
      "25%              2001-07-16 00:00:00    36.910000    27.784625    10.000000   \n",
      "50%              2011-11-07 12:00:00    38.086900    31.825000    10.000000   \n",
      "75%              2021-04-17 00:00:00    39.100000    38.676500    16.000000   \n",
      "max              2025-01-21 00:00:00    42.494000    45.190000   176.100000   \n",
      "std                              NaN     1.548517     6.212623    26.697618   \n",
      "\n",
      "               mag         year        month          day       target  \n",
      "count  4128.000000  4128.000000  4128.000000  4128.000000  4128.000000  \n",
      "mean      4.383866  2010.291667     6.048207    15.408188     0.172481  \n",
      "min       4.000000  1990.000000     1.000000     1.000000     0.000000  \n",
      "25%       4.100000  2001.000000     2.000000     7.000000     0.000000  \n",
      "50%       4.300000  2011.000000     6.000000    15.000000     0.000000  \n",
      "75%       4.500000  2021.000000     9.000000    23.000000     0.000000  \n",
      "max       7.800000  2025.000000    12.000000    31.000000     1.000000  \n",
      "std       0.404915    10.703358     3.586293     8.777259     0.377843  \n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:36.370629Z",
     "start_time": "2025-01-24T16:53:36.366527Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "b02d12b25a6b223d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4128 entries, 0 to 4127\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   time       4128 non-null   datetime64[ns]\n",
      " 1   latitude   4128 non-null   float64       \n",
      " 2   longitude  4128 non-null   float64       \n",
      " 3   depth      4128 non-null   float64       \n",
      " 4   mag        4128 non-null   float64       \n",
      " 5   year       4128 non-null   int32         \n",
      " 6   month      4128 non-null   int32         \n",
      " 7   day        4128 non-null   int32         \n",
      " 8   target     4128 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int32(3), int64(1)\n",
      "memory usage: 242.0 KB\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:38.059433Z",
     "start_time": "2025-01-24T16:53:36.371219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = find_recent_earthquakes(df)\n",
    "\n",
    "print(df[\"close_event\"].value_counts())"
   ],
   "id": "d87cca67a6f0e498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close_event\n",
      "0     3094\n",
      "1      325\n",
      "2      164\n",
      "3       90\n",
      "4       67\n",
      "      ... \n",
      "66       1\n",
      "65       1\n",
      "61       1\n",
      "64       1\n",
      "17       1\n",
      "Name: count, Length: 61, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:38.063807Z",
     "start_time": "2025-01-24T16:53:38.060481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_largest_nearby_eq(data, mag_col='mag', lat_col='latitude', lon_col='longitude', time_col='time'):\n",
    "    \"\"\"\n",
    "    Add a feature `largest_nearby_eq` indicating the magnitude of the largest earthquake\n",
    "    within a 0.5-degree radius and 20-year history for each record in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Earthquake dataset with columns for magnitude, latitude, longitude, and time.\n",
    "    mag_col : str\n",
    "        Column name for magnitude.\n",
    "    lat_col : str\n",
    "        Column name for latitude.\n",
    "    lon_col : str\n",
    "        Column name for longitude.\n",
    "    time_col : str\n",
    "        Column name for time.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with an added column `largest_nearby_eq`.\n",
    "    \"\"\"\n",
    "    # Convert time column to datetime if not already\n",
    "    data[time_col] = pd.to_datetime(data[time_col])\n",
    "    data = data.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    # Initialize the new feature\n",
    "    largest_eq = np.zeros(len(data))\n",
    "\n",
    "    # Iterate over each earthquake\n",
    "    for i, row in data.iterrows():\n",
    "        current_time = row[time_col]\n",
    "        current_lat, current_lon = row[lat_col], row[lon_col]\n",
    "\n",
    "        # Filter earthquakes within the last 20 years but before the current event\n",
    "        recent_data = data[\n",
    "            (data[time_col] < current_time) &\n",
    "            (data[time_col] >= current_time - pd.Timedelta(days=20 * 365))\n",
    "        ]\n",
    "\n",
    "        # Filter by proximity (latitude and longitude)\n",
    "        nearby_quakes = recent_data[\n",
    "            (np.abs(recent_data[lat_col] - current_lat) <= 0.5) &\n",
    "            (np.abs(recent_data[lon_col] - current_lon) <= 0.5)\n",
    "        ]\n",
    "\n",
    "        # Find the largest earthquake magnitude\n",
    "        largest_eq[i] = nearby_quakes[mag_col].max() if not nearby_quakes.empty else 0\n",
    "\n",
    "    # Add the feature to the DataFrame\n",
    "    data[\"largest_nearby_eq\"] = largest_eq\n",
    "    return data"
   ],
   "id": "40f241a1ccb1851",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:39.672301Z",
     "start_time": "2025-01-24T16:53:38.064417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = add_largest_nearby_eq(df)\n",
    "\n",
    "print(df[\"largest_nearby_eq\"].value_counts())\n"
   ],
   "id": "da6176588dd20376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest_nearby_eq\n",
      "6.0    325\n",
      "5.5    265\n",
      "5.6    244\n",
      "5.2    210\n",
      "4.8    194\n",
      "7.1    179\n",
      "5.1    167\n",
      "4.7    163\n",
      "6.2    154\n",
      "4.9    140\n",
      "4.6    140\n",
      "6.4    138\n",
      "5.7    135\n",
      "0.0    133\n",
      "5.9    125\n",
      "4.5    122\n",
      "6.1    112\n",
      "6.7    105\n",
      "5.4     98\n",
      "4.1     93\n",
      "5.8     92\n",
      "5.0     91\n",
      "4.4     90\n",
      "5.3     88\n",
      "6.3     78\n",
      "6.6     72\n",
      "4.2     66\n",
      "7.5     62\n",
      "7.0     56\n",
      "4.3     53\n",
      "7.8     45\n",
      "7.2     36\n",
      "4.0     21\n",
      "6.5     20\n",
      "7.6     16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:39.676523Z",
     "start_time": "2025-01-24T16:53:39.673255Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "93607c1d29137ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4128 entries, 0 to 4127\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   time               4128 non-null   datetime64[ns]\n",
      " 1   latitude           4128 non-null   float64       \n",
      " 2   longitude          4128 non-null   float64       \n",
      " 3   depth              4128 non-null   float64       \n",
      " 4   mag                4128 non-null   float64       \n",
      " 5   year               4128 non-null   int32         \n",
      " 6   month              4128 non-null   int32         \n",
      " 7   day                4128 non-null   int32         \n",
      " 8   target             4128 non-null   int64         \n",
      " 9   close_event        4128 non-null   int64         \n",
      " 10  largest_nearby_eq  4128 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(5), int32(3), int64(2)\n",
      "memory usage: 306.5 KB\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:44.005884Z",
     "start_time": "2025-01-24T16:53:40.590701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_time_based_sequences(data, window_days, features, train_split=0.8, \n",
    "                               min_events=5, handling_method='skip'):\n",
    "    \"\"\"\n",
    "    Prepare sequences for LSTM model based on time windows with proper handling of initial sequences.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing earthquake data (must have 'time' column)\n",
    "        window_days: Number of days to look back for each sequence\n",
    "        features: List of feature columns to use\n",
    "        train_split: Proportion of data to use for training (default: 0.8)\n",
    "        min_events: Minimum number of events required in a window\n",
    "        handling_method: How to handle sequences with insufficient data\n",
    "                        'skip': Skip sequences with insufficient data\n",
    "                        'pad': Pad sequences with zeros\n",
    "                        'minimum_window': Use available data if it meets minimum requirements\n",
    "    \n",
    "    Returns:\n",
    "        train_X, train_y, test_X, test_y, train_mask, test_mask, sequence_stats\n",
    "    \"\"\"\n",
    "    # Sort data by time to ensure temporal order\n",
    "    data = data.sort_values('time').reset_index(drop=True)\n",
    "    \n",
    "    # Initialize lists for sequences\n",
    "    X = []\n",
    "    y = []\n",
    "    sequence_lengths = []\n",
    "    dates = []\n",
    "    skipped_sequences = 0\n",
    "    \n",
    "    # Calculate the start of the dataset\n",
    "    dataset_start = data['time'].iloc[0]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        end_time = data['time'].iloc[i]\n",
    "        start_time = end_time - pd.Timedelta(days=window_days)\n",
    "        \n",
    "        # Get all earthquakes within the time window\n",
    "        mask = (data['time'] < end_time) & (data['time'] >= start_time)\n",
    "        window_data = data[mask][features].values\n",
    "        \n",
    "        # Handle different cases based on the specified method\n",
    "        if handling_method == 'skip':\n",
    "            if len(window_data) >= min_events:\n",
    "                X.append(window_data)\n",
    "                y.append(data['target'].iloc[i])\n",
    "                sequence_lengths.append(len(window_data))\n",
    "                dates.append(end_time)\n",
    "            else:\n",
    "                skipped_sequences += 1\n",
    "                \n",
    "        elif handling_method == 'pad':\n",
    "            if len(window_data) > 0:\n",
    "                X.append(window_data)\n",
    "                y.append(data['target'].iloc[i])\n",
    "                sequence_lengths.append(len(window_data))\n",
    "                dates.append(end_time)\n",
    "                \n",
    "        elif handling_method == 'minimum_window':\n",
    "            # If we're near the start of the dataset, use available data\n",
    "            if start_time < dataset_start:\n",
    "                available_data = data[data['time'] < end_time][features].values\n",
    "                if len(available_data) >= min_events:\n",
    "                    X.append(available_data)\n",
    "                    y.append(data['target'].iloc[i])\n",
    "                    sequence_lengths.append(len(available_data))\n",
    "                    dates.append(end_time)\n",
    "                else:\n",
    "                    skipped_sequences += 1\n",
    "            else:\n",
    "                if len(window_data) >= min_events:\n",
    "                    X.append(window_data)\n",
    "                    y.append(data['target'].iloc[i])\n",
    "                    sequence_lengths.append(len(window_data))\n",
    "                    dates.append(end_time)\n",
    "                else:\n",
    "                    skipped_sequences += 1\n",
    "    \n",
    "    # Convert dates to numpy datetime64\n",
    "    dates = np.array(dates)\n",
    "    \n",
    "    # Find maximum sequence length for padding\n",
    "    max_seq_length = max(sequence_lengths)\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    X_padded = []\n",
    "    for seq in X:\n",
    "        if len(seq) < max_seq_length:\n",
    "            padding = np.zeros((max_seq_length - len(seq), len(features)))\n",
    "            seq_padded = np.vstack((padding, seq))  # Pad at the beginning\n",
    "        else:\n",
    "            seq_padded = seq[-max_seq_length:]  # Take the most recent events\n",
    "        X_padded.append(seq_padded)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X_padded)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Calculate split index based on time\n",
    "    split_time = dates[0] + (dates[-1] - dates[0]) * train_split\n",
    "    split_idx = sum([1 for seq_time in dates if seq_time <= split_time])\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_X = X[:split_idx]\n",
    "    test_X = X[split_idx:]\n",
    "    train_y = y[:split_idx]\n",
    "    test_y = y[split_idx:]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_X = torch.FloatTensor(train_X)\n",
    "    test_X = torch.FloatTensor(test_X)\n",
    "    train_y = torch.LongTensor(train_y)\n",
    "    test_y = torch.LongTensor(test_y)\n",
    "    \n",
    "    # Create mask for padded sequences\n",
    "    train_mask = (train_X != 0).any(dim=2)\n",
    "    test_mask = (test_X != 0).any(dim=2)\n",
    "    \n",
    "    # Compile sequence statistics\n",
    "    sequence_stats = {\n",
    "        'total_sequences': len(X),\n",
    "        'skipped_sequences': skipped_sequences,\n",
    "        'max_sequence_length': max_seq_length,\n",
    "        'min_sequence_length': min(sequence_lengths),\n",
    "        'avg_sequence_length': sum(sequence_lengths) / len(sequence_lengths),\n",
    "        'train_sequences': len(train_X),\n",
    "        'test_sequences': len(test_X)\n",
    "    }\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, train_mask, test_mask, sequence_stats\n",
    "\n",
    "# Example usage:\n",
    "window_days = 30\n",
    "min_events = 5\n",
    "\n",
    "features = ['latitude', 'longitude', 'depth', 'mag', 'year', 'month', 'day', 'close_event', 'largest_nearby_eq']\n",
    "\n",
    "# Try different handling methods\n",
    "methods = ['skip', 'pad', 'minimum_window']\n",
    "for method in methods:\n",
    "    print(f\"\\nTrying method: {method}\")\n",
    "    train_X, train_y, test_X, test_y, train_mask, test_mask, stats = prepare_time_based_sequences(\n",
    "        data=df,\n",
    "        window_days=window_days,\n",
    "        features=features,\n",
    "        train_split=0.8,\n",
    "        min_events=min_events,\n",
    "        handling_method=method\n",
    "    )\n",
    "    \n",
    "    print(\"Sequence Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")"
   ],
   "id": "c41f5e0eaedf2ca9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying method: skip\n",
      "Sequence Statistics:\n",
      "total_sequences: 3426\n",
      "skipped_sequences: 702\n",
      "max_sequence_length: 470\n",
      "min_sequence_length: 5\n",
      "avg_sequence_length: 42.5758902510216\n",
      "train_sequences: 2073\n",
      "test_sequences: 1353\n",
      "\n",
      "Trying method: pad\n",
      "Sequence Statistics:\n",
      "total_sequences: 4110\n",
      "skipped_sequences: 0\n",
      "max_sequence_length: 470\n",
      "min_sequence_length: 1\n",
      "avg_sequence_length: 35.976885644768856\n",
      "train_sequences: 2673\n",
      "test_sequences: 1437\n",
      "\n",
      "Trying method: minimum_window\n",
      "Sequence Statistics:\n",
      "total_sequences: 3426\n",
      "skipped_sequences: 702\n",
      "max_sequence_length: 470\n",
      "min_sequence_length: 5\n",
      "avg_sequence_length: 42.5758902510216\n",
      "train_sequences: 2073\n",
      "test_sequences: 1353\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:47.081286Z",
     "start_time": "2025-01-24T16:53:45.939055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X, train_y, test_X, test_y, train_mask, test_mask, stats = prepare_time_based_sequences(\n",
    "    data=df,\n",
    "    window_days=30,\n",
    "    features=['latitude', 'longitude', 'depth', 'mag', 'year', 'month', 'day', 'close_event', 'largest_nearby_eq'],\n",
    "    train_split=0.8,\n",
    "    min_events=5,\n",
    "    handling_method='minimum_window'\n",
    ")"
   ],
   "id": "8d855b6f1654035f",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:53:47.144837Z",
     "start_time": "2025-01-24T16:53:47.082165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
    "test_X_scaled = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)"
   ],
   "id": "cbb3956d77a40890",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:54:36.041675Z",
     "start_time": "2025-01-24T16:53:48.177694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def train_evaluate_model(train_X, train_y, test_X, test_y, train_mask, test_mask, features, num_epochs=50):\n",
    "    batch_size = 64\n",
    "\n",
    "    # Create datasets with masks\n",
    "    train_dataset = TensorDataset(train_X, train_y, train_mask)\n",
    "    test_dataset = TensorDataset(test_X, test_y, test_mask)\n",
    "\n",
    "    # Create data loaders (no shuffle to maintain temporal order)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(train_y.numpy().astype(int))\n",
    "    pos_weight = class_counts[0] / class_counts[1]\n",
    "    pos_weight = torch.tensor([pos_weight], dtype=torch.float).to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Model parameters\n",
    "    input_size = len(features)\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1  # Binary classification\n",
    "\n",
    "    # Initialize model\n",
    "    model = EarthquakeLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for sequences_batch, labels_batch, mask_batch in train_loader:\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device).float().unsqueeze(1)\n",
    "            mask_batch = mask_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences_batch, mask_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences_batch, labels_batch, mask_batch in test_loader:\n",
    "                sequences_batch = sequences_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device).float().unsqueeze(1)\n",
    "                mask_batch = mask_batch.to(device)\n",
    "\n",
    "                outputs = model(sequences_batch, mask_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "        for sequences_batch, labels_batch, mask_batch in test_loader:\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            mask_batch = mask_batch.to(device)\n",
    "\n",
    "            outputs = model(sequences_batch, mask_batch)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_outputs = np.array(all_outputs).squeeze()\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Binarize outputs\n",
    "    threshold = 0.5\n",
    "    predicted = (all_outputs >= threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, predicted)\n",
    "    precision = precision_score(all_labels, predicted, zero_division=0)\n",
    "    recall = recall_score(all_labels, predicted, zero_division=0)\n",
    "    f1 = f1_score(all_labels, predicted, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels, predicted)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nFinal Evaluation Metrics:\")\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Calculate and print additional metrics\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    print(f'\\nSpecificity: {specificity:.4f}')\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_outputs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "    return model, {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'specificity': specificity,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "train_X, train_y, test_X, test_y, train_mask, test_mask, stats = prepare_time_based_sequences(\n",
    "    data=df,\n",
    "    window_days=30,\n",
    "    features=features,\n",
    "    train_split=0.8,\n",
    "    min_events=5,\n",
    "    handling_method='minimum_window'\n",
    ")\n",
    "\n",
    "model, metrics = train_evaluate_model(\n",
    "    train_X=train_X,\n",
    "    train_y=train_y,\n",
    "    test_X=test_X,\n",
    "    test_y=test_y,\n",
    "    train_mask=train_mask,\n",
    "    test_mask=test_mask,\n",
    "    features=features,\n",
    "    num_epochs=50\n",
    ")"
   ],
   "id": "aaa307b49617bbfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/50], Train Loss: 1.2113, Val Loss: 1.6788\n",
      "Epoch [2/50], Train Loss: 1.1909, Val Loss: 1.6599\n",
      "Epoch [3/50], Train Loss: 1.1885, Val Loss: 1.6506\n",
      "Epoch [4/50], Train Loss: 1.1882, Val Loss: 1.6439\n",
      "Epoch [5/50], Train Loss: 1.1888, Val Loss: 1.6393\n",
      "Epoch [6/50], Train Loss: 1.1873, Val Loss: 1.6362\n",
      "Epoch [7/50], Train Loss: 1.1868, Val Loss: 1.6333\n",
      "Epoch [8/50], Train Loss: 1.1875, Val Loss: 1.6312\n",
      "Epoch [9/50], Train Loss: 1.1865, Val Loss: 1.6299\n",
      "Epoch [10/50], Train Loss: 1.1868, Val Loss: 1.6291\n",
      "Epoch [11/50], Train Loss: 1.1868, Val Loss: 1.6280\n",
      "Epoch [12/50], Train Loss: 1.1870, Val Loss: 1.6272\n",
      "Epoch [13/50], Train Loss: 1.1863, Val Loss: 1.6264\n",
      "Epoch [14/50], Train Loss: 1.1865, Val Loss: 1.6255\n",
      "Epoch [15/50], Train Loss: 1.1862, Val Loss: 1.6252\n",
      "Epoch [16/50], Train Loss: 1.1862, Val Loss: 1.6255\n",
      "Epoch [17/50], Train Loss: 1.1860, Val Loss: 1.6248\n",
      "Epoch [18/50], Train Loss: 1.1856, Val Loss: 1.6240\n",
      "Epoch [19/50], Train Loss: 1.1855, Val Loss: 1.6233\n",
      "Epoch [20/50], Train Loss: 1.1854, Val Loss: 1.6232\n",
      "Epoch [21/50], Train Loss: 1.1859, Val Loss: 1.6230\n",
      "Epoch [22/50], Train Loss: 1.1857, Val Loss: 1.6230\n",
      "Epoch [23/50], Train Loss: 1.1857, Val Loss: 1.6228\n",
      "Epoch [24/50], Train Loss: 1.1849, Val Loss: 1.6216\n",
      "Epoch [25/50], Train Loss: 1.1856, Val Loss: 1.6241\n",
      "Epoch [26/50], Train Loss: 1.1860, Val Loss: 1.6217\n",
      "Epoch [27/50], Train Loss: 1.1859, Val Loss: 1.6202\n",
      "Epoch [28/50], Train Loss: 1.1852, Val Loss: 1.6206\n",
      "Epoch [29/50], Train Loss: 1.1853, Val Loss: 1.6212\n",
      "Epoch [30/50], Train Loss: 1.1852, Val Loss: 1.6214\n",
      "Epoch [31/50], Train Loss: 1.1848, Val Loss: 1.6218\n",
      "Epoch [32/50], Train Loss: 1.1853, Val Loss: 1.6212\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_l/h44zx0_s3pd47gnlplq87_1h0000gn/T/ipykernel_32926/513067067.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Metrics:\n",
      "Accuracy: 0.7332\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[992   0]\n",
      " [361   0]]\n",
      "\n",
      "Specificity: 1.0000\n",
      "ROC AUC: 0.7655\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cde3f948edc4d0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
